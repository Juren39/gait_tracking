import argparse
import cv2
import numpy as np
from functools import partial
from pathlib import Path
import yaml
import json
import torch
import subprocess
from tqdm import tqdm

from boxmot import TRACKERS
from boxmot.tracker_zoo import create_tracker
from boxmot.utils import ROOT, WEIGHTS, TRACKER_CONFIGS
from boxmot.utils.checks import RequirementsChecker
from tracking.detectors import (get_yolo_inferer, default_imgsz,
                                is_ultralytics_model, is_yolox_model)

checker = RequirementsChecker()
checker.check_packages(('ultralytics @ git+https://github.com/mikel-brostrom/ultralytics.git', ))  # install

from ultralytics import YOLO
from ultralytics.utils.plotting import Annotator, colors
from ultralytics.data.utils import VID_FORMATS
from ultralytics.utils.plotting import save_one_box

from dataset.data_store import load_existing_id_mapping,save_id_mapping,get_track_number_by_id

def on_predict_start(predictor, persist=False):
    assert predictor.custom_args['tracking_method'] in TRACKERS, \
        f"'{predictor.custom_args['tracking_method']}' is not supported. Supported ones are {TRACKERS}"

    tracking_config = TRACKER_CONFIGS / (predictor.custom_args['tracking_method'] + '.yaml')
    trackers = []
    for i in range(predictor.dataset.bs):
        tracker = create_tracker(
            predictor.custom_args['tracking_method'],
            tracking_config,
            predictor.custom_args['reid_model'],
            predictor.custom_args['gait_model'],
            predictor.device,
            predictor.custom_args['half'],
            predictor.custom_args['save_file'],
            predictor.custom_args['mode'],
            predictor.custom_args['per_class']
        )
        if hasattr(tracker, 'model'):
            tracker.model.warmup()
        trackers.append(tracker)

    predictor.trackers = trackers

@torch.no_grad()
def run(args):
    yolo_model = args['yolo_model']
    reid_model = args['reid_model']
    gait_model = args['gait_model']
    tracking_method = args['tracking_method']
    source = args['source']
    imgsz = args['imgsz'] or default_imgsz(yolo_model)
    conf = args['conf']
    iou = args['iou']
    device = args['device']
    show = args['show']
    classes = args['classes']
    name = args['name']
    exist_ok = args['exist_ok']
    half = args['half']
    vid_stride = args['vid_stride']
    show_labels = args['show_labels']
    show_conf = args['show_conf']
    show_trajectories = args['show_trajectories']
    save_id_crops = args['save_id_crops']
    line_width = args['line_width']
    per_class = args['per_class']
    agnostic_nms = args['agnostic_nms']
    verbose = args['verbose']
    videofps = args['videofps']
    save_file = args['save_file']
    mode = args['mode']

    if mode == "registration":
        source_path = Path(source) / 'videos'
    else:
        source_path = Path(source)
    if source_path.is_dir():
        video_files = []
        for ext in ("*.mp4", "*.avi"):
            video_files.extend(source_path.glob(ext))
    else:
        video_files = [source_path]

    registered_ids = set()  
    id_info_list = []  # è¯¦ç»†ä¿¡æ¯

    for video_path in video_files:

        source_file_name = video_path.stem
        video_output_path = Path('./output') / name / 'visualization' / f'{source_file_name}_tracked.mp4'
        label_output_path = Path('./output') / name / 'labels' / f'{source_file_name}_labels.txt'
        origin_to_mp4_path = Path('./output') / name / 'videos' / f'{source_file_name}.mp4'
        mapping_file = Path('./dataset/id_name_mapping.txt')
        mapping_file.parent.mkdir(parents=True, exist_ok=True)
        video_output_path.parent.mkdir(parents=True, exist_ok=True)
        label_output_path.parent.mkdir(parents=True, exist_ok=True)
        origin_to_mp4_path.parent.mkdir(parents=True, exist_ok=True)

        convert_to_mp4(video_path, origin_to_mp4_path, videofps)

        vid = cv2.VideoCapture(origin_to_mp4_path)
        frame_width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(vid.get(cv2.CAP_PROP_FPS))
        total_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))
        vid.release()
        
        if mode == "registration":
            tracking_config = TRACKER_CONFIGS / (args['tracking_method'] + '.yaml')
            tracker = create_tracker(
                tracker_type=args['tracking_method'],
                tracker_config=tracking_config,  
                reid_weights=args['reid_model'],
                gait_weights=args['gait_model'],
                device=device,
                half=args['half'],
                save_file=save_file,
                mode=mode,
                per_class=args['per_class'],
            )
            cap = cv2.VideoCapture(str(origin_to_mp4_path))
            fps = cap.get(cv2.CAP_PROP_FPS)

            progress_bar = (
                tqdm(total=total_frames, desc="Processing frames", unit="frame", dynamic_ncols=True, leave=True)
                if ~verbose else None
            )
            id_map, max_idx = load_existing_id_mapping(mapping_file)
            frames_list = []
            dets_list = []
            frame_idx = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                frames_list.append(frame)
                dets, id_list = load_json_bboxes(Path(source) / 'labels' / f'{source_file_name}_labels.txt', frame_idx)  # [x1, y1, x2, y2, confidence, cls, track_id]
                # éå†æ¯ä¸ªæ£€æµ‹æ¡†ï¼Œè·å– track_id
                for i, box in enumerate(dets):
                    track_id = id_list[i]
                    # è‹¥è¯¥ track_id æœªåœ¨æ˜ å°„è¡¨é‡Œï¼Œåˆ™åˆ†é…æ–°çš„é¡ºåºç¼–å·
                    if track_id not in id_map:
                        max_idx += 1
                        id_map[track_id] = max_idx
                new_column = np.array([[id_map[id_list[i]]] for i in range(len(dets))]).reshape(-1, 1)  # (N, 1)
                dets = np.hstack((dets, new_column))  # (N, M+1)
                dets_list.append(dets)
                # å‡è®¾è¿”å› (N,7): [x1,y1,x2,y2,score,cls,track_id]
                progress_bar.update(1)
                frame_idx += 1
            progress_bar.close()
            save_id_mapping(mapping_file, id_map)
            print('registration start!')
            box_id_list = tracker.registration(dets_list, frames_list)
            # ç»Ÿè®¡å‡ºç°çš„ ID
            for track_id in box_id_list:
                if track_id not in registered_ids:
                    registered_ids.add(track_id)
                    id_info_list.append({
                        "track_id": track_id,
                    })
            cap.release()
            print('registration end!')

        else:
            if imgsz is None:
                imgsz = default_imgsz(yolo_model)

            yolo = YOLO(
                yolo_model if is_ultralytics_model(yolo_model)
                else 'yolov8n.pt',
            )

            results = yolo.track(
                source=origin_to_mp4_path,
                conf=conf,
                iou=iou,
                agnostic_nms=agnostic_nms,
                show=False,
                stream=True,
                device=device,
                show_conf=show_conf,
                show_labels=show_labels,
                exist_ok=exist_ok,
                name=name,
                classes=classes,
                imgsz=imgsz,       
                vid_stride=vid_stride,
                line_width=line_width,
                verbose=verbose
            )
            frame_idx = 0  # è®°å½•å¸§ç¼–å· 

            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(str(video_output_path), fourcc, fps, (frame_width, frame_height))
            yolo.add_callback('on_predict_start', partial(on_predict_start, persist=True))

            if not is_ultralytics_model(yolo_model):
                # replace yolov8 model
                m = get_yolo_inferer(yolo_model)
                yolo_model = m(model=yolo_model, device=yolo.predictor.device,
                               args=yolo.predictor.args)
                yolo.predictor.model = yolo_model

                # If current model is YOLOX, change the preprocess and postprocess
                if is_yolox_model(yolo_model):
                    # add callback to save image paths for further processing
                    yolo.add_callback("on_predict_batch_start",
                                      lambda p: yolo_model.update_im_paths(p))
                    yolo.predictor.preprocess = (
                        lambda imgs: yolo_model.preprocess(im=imgs))
                    yolo.predictor.postprocess = (
                        lambda preds, im, im0s:
                        yolo_model.postprocess(preds=preds, im=im, im0s=im0s))

            yolo.predictor.custom_args = args
            labels = {
                "video_name": source_file_name,
                "fps": fps,
                "width": frame_width,
                "height": frame_height,
                "frames": []
            }
            progress_bar = (
                tqdm(total=total_frames, desc="Processing frames", unit="frame", dynamic_ncols=True, leave=True)
                if ~verbose else None
            )
            for r in results:
                tracks_info = []
                img = yolo.predictor.trackers[0].plot_results(r.orig_img, show_trajectories)
                for box in r.boxes: # [d, [trk.id], [trk.conf], [trk.cls]]
                   # åæ ‡
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf_val = float(box.conf[0].cpu().item())
                    cls_val = int(box.cls[0].cpu().item())
                    # track_id å¯èƒ½ä¸ºç©ºï¼Œéœ€è¦åšåˆ¤æ–­
                    if box.id is not None:
                        track_id = int(box.id[0].item())
                    else:
                        track_id = -1    
                    tracks_info.append({
                        "track_id": track_id,
                        "class_id": cls_val,
                        "confidence": conf_val,
                        "bbox": [float(x1), float(y1), float(x2), float(y2)]
                    })
                # å°†è¯¥å¸§çš„ä¿¡æ¯å­˜å…¥ labels
                labels["frames"].append({
                    "frame_id": frame_idx,
                    "detections": tracks_info
                })
                if ~verbose:
                    num_people = len(tracks_info)
                    progress_bar.set_postfix({"Frame": frame_idx, "People Detected": num_people})
                    progress_bar.update(1)
                    
                frame_idx += 1
                out.write(img)
            progress_bar.close()
            out.release()
            
            with open(label_output_path, 'w', encoding='utf-8') as f:
                json.dump(labels, f, ensure_ascii=False, indent=4)
            print(f"video file saved to: {video_output_path.resolve()}")
            print(f"label file saved to: {label_output_path.resolve()}")
    
    if mode == 'registration':
        print(f"ğŸ“Œ Registration æ¨¡å¼å®Œæˆï¼Œå…±æ³¨å†Œ {len(registered_ids)} ä¸ª ID")
        print("ğŸ“‹ è¯¦ç»†ä¿¡æ¯:")
        for info in id_info_list:
            print(f"ğŸ”¹ ID :{info['track_id']}")
    else:
        print(f"ğŸ“Œ Recognition æ¨¡å¼å®Œæˆï¼Œå…±æ£€æµ‹ {len(video_files)} ä¸ªè§†é¢‘")
    
def convert_to_mp4(input_path, output_path, videofps):

    command = [
        "ffmpeg",
        '-n',
        "-i", input_path,           # è¾“å…¥è§†é¢‘
        "-c:v", "libx264",          # è§†é¢‘ç¼–ç å™¨
        "-preset", "fast",          # å‹ç¼©é€Ÿåº¦/è´¨é‡å¹³è¡¡
        "-crf", "23",               # è§†é¢‘è´¨é‡ï¼ˆæ•°å€¼è¶Šå°è¶Šæ¸…æ™°ï¼‰
        "-c:a", "aac",              # éŸ³é¢‘ç¼–ç å™¨
        "-b:a", "128k",             # éŸ³é¢‘æ¯”ç‰¹ç‡
        "-movflags", "+faststart",  # ä¼˜åŒ–æ–‡ä»¶å¤´
        "-r", videofps,
        output_path
    ]
    dataset_path = Path("./dataset/ffmpeg_log.txt")
    dataset_path.parent.mkdir(parents=True, exist_ok=True)
    with open("./dataset/ffmpeg_log.txt", "w") as log_file:
        subprocess.run(command, stdout=log_file, stderr=log_file)

def load_json_bboxes(json_path: str, frame_idx: int) -> np.ndarray: #mainæ–¹æ³•ä¸­çš„load_json_bboxeså¯¹åº”çš„track_idä»¥åŠidé—®é¢˜ï¼Œéœ€è¦ä¿®å¤annotationæ¨¡å¼æ ‡æ³¨å‡ºæ¥çš„æ–‡æœ¬ã€‚
    """
    ä»ä¸Šè¿° JSON æ–‡ä»¶ä¸­è¯»å–æŒ‡å®š frame_idx çš„æ£€æµ‹æ¡†ä¿¡æ¯ï¼Œè¿”å›å½¢å¦‚ (N, 6) çš„ array:
      [x1, y1, x2, y2, score, cls]
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    frames = data['frames']
    cur_frame_data = None
    for fdata in frames:
        if fdata['frame_id'] == frame_idx:
            cur_frame_data = fdata
            break
    if cur_frame_data is None:
        return np.empty((0, 6))
    
    # æå– detections
    detections = cur_frame_data.get('detections', [])
    bboxes = []
    id_list = []
    for det in detections:
        # è§£æ
        confidence = float(det['confidence'])
        cls = float(det['class_id'])  # å¦‚æœ class_id å…¨æ˜¯ intï¼Œå¯ä»¥è½¬æˆ float32
        x1, y1, x2, y2 = det['bbox']  # bboxæ˜¯list: [x1, y1, x2, y2]
        bboxes.append([x1, y1, x2, y2, confidence, cls])
        id_list.append(det['track_id'])

    if len(bboxes) == 0:
        return np.empty((0, 7), dtype=np.float32), id_list
    
    return np.array(bboxes, dtype=np.float32), id_list

def draw_tracking_results(image, tracks):
    for track in tracks:
        x1, y1, x2, y2, track_id = track[:5]
        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        cv2.putText(image, f"ID: {int(track_id)}", (int(x1), int(y1) - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
def parse_opt():
    """
    ä» YAML æ–‡ä»¶è¯»å–é…ç½®å‚æ•°ã€‚
    """
    with open('./config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    config['yolo_model'] = Path(config['yolo_model'])  
    config['reid_model'] = Path(config['reid_model'])  
    config['gait_model'] = Path(config['gait_model']) 
    config['tracking_method'] = str(config['tracking_method'])  
    config['source'] = str(config['source'])  
    config['imgsz'] = list(map(int, config['imgsz'])) if isinstance(config['imgsz'], list) else config['imgsz']  
    config['conf'] = float(config['conf']) 
    config['iou'] = float(config['iou']) 
    config['device'] = str(config['device'])  
    config['show'] = bool(config['show'])  
    config['classes'] = list(map(int, config['classes'])) if config['classes'] is not None else None 
    config['name'] = str(config['name']) 
    config['exist_ok'] = bool(config['exist_ok'])  
    config['half'] = bool(config['half'])  
    config['vid_stride'] = int(config['vid_stride']) 
    config['show_labels'] = bool(config['show_labels'])  
    config['show_conf'] = bool(config['show_conf'])  
    config['show_trajectories'] = bool(config['show_trajectories'])  
    config['save_id_crops'] = bool(config['save_id_crops'])  
    config['line_width'] = int(config['line_width']) if config['line_width'] is not None else None  
    config['per_class'] = bool(config['per_class'])   
    config['agnostic_nms'] = bool(config['agnostic_nms'])  
    config['verbose'] = bool(config['verbose'])
    config['videofps'] = str(config['videofps'])
    config['save_file'] = Path(config['save_file'])
    config['mode'] = str(config['mode'])

    return config

if __name__ == "__main__":
    config = parse_opt()
    run(config)